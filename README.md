# ğŸš€ DistriRAG Framework
### Distributed, Agnostic, and Deterministic RAG Architecture

---

## ğŸ“‹ 1. Executive Summary

**DistriRAG** is a high-performance **RAG (Retrieval-Augmented Generation)** ecosystem designed for enterprise-grade scalability and strict data privacy. By decoupling the ingestion, vector indexing, and inference layers, it provides a unique **"Hybrid-Edge"** approach where data stays secure and compute remains flexible.

---

## ğŸ—ï¸ 2. Technical Architecture & Stack

- **ğŸ§  Orchestration Layer (The Brain):**  
  **n8n** manages the complex logic flow, acting as a stateless API Gateway and infrastructure controller.

- **ğŸ’¾ Data Consistency Layer:**  
  **Postgres 16** manages relational metadata and system state, ensuring 1:1 synchronization with the vector store.

- **ğŸ” Vector Engine:**  
  **Qdrant** provides high-speed HNSW indexing for semantic retrieval.

- **âš™ï¸ Service Layer:**  
  **FastAPI (Python)** handles heavy lifting like PDF coordinate extraction, embedding generation, and semantic reranking.

- **ğŸ›¡ï¸ Application Layer:**  
  **NestJS** provides a robust backend for user management and system configuration.

- **ğŸ¨ Presentation Layer:**  
  **Angular** offers a specialized UI featuring "Point-and-Click" source verification.

---

## ğŸ’¡ 3. Key Innovation: Deterministic Source Traceability

Most RAG systems suffer from "hallucinated references." **DistriRAG** solves this by:

1. **Extracting exact bounding box coordinates** during PDF ingestion.
2. **Mapping unique Chunk-UUIDs** across Postgres and Qdrant.
3. **Allowing the user to click** an AI-generated citation and instantly open the source document at the exact page and paragraph.

---

## ğŸŒ 4. Hybrid-Edge Operations (SSH Orchestration)

Through a custom **SSH2-based node in n8n**, the system can:

- **Provision and manage** remote inference engines (Ollama/vLLM) on edge GPU nodes.
- **Separate** the sensitive data layer from the high-compute inference layer.
- **Switch dynamically** between Cloud providers (Google Vertex AI, OpenAI) and local hardware based on cost or privacy requirements.

---

## ğŸš¢ 5. Deployment Strategy

The entire stack is **containerized using Docker Compose**, supporting multi-node deployment. Each microservice is independently scalable, allowing for high-availability configurations in production environments.
